{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UNESCO 'World Heritage List' can be found online: [UNESCO Homepage](https://whc.unesco.org), [XML File](https://whc.unesco.org/en/list/xml) \\\n",
    "**Copyright © 1992 - 2024 UNESCO/World Heritage Centre. All rights reserved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file directly\n",
    "file = urllib.request.urlopen('https://whc.unesco.org/en/list/xml') \n",
    "raw_xml = file.read().decode('utf8')\n",
    "file.close()\n",
    "\n",
    "sites = pd.read_xml(io.StringIO(raw_xml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or access from pre-downloaded file\n",
    "with open('raw-files/whc-en.xml', 'r', encoding=\"utf8\") as f:\n",
    "    raw_xml = f.read()\n",
    "    \n",
    "sites = pd.read_xml(io.StringIO(raw_xml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "## Text Columns\n",
    "- Strings of comma separated lists to be converted to list datatype\n",
    "- HTML to be converted to plaintext\n",
    "- Criteria text to be converted to multiple columns '(i)(iii)' -> C1 = True, C2 = False, C3 = True, C4-10 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_sep_cols = [\"iso_code\", \"states\", \"secondary_dates\"]\n",
    "html_cols = [\"site\", \"short_description\", \"justification\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comma-Separated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colname in comma_sep_cols:\n",
    "    sites[colname] = sites[colname].str.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Columns\n",
    "Initial implementation of a HTML stripping function was attempted using regex substitutions for tags. However, many special characters (e.g. \"\\&ndash;\") would slip through the cracks.\n",
    "\n",
    "Mapping these characters manually was inflexible and led me to the BeautifulSoup & unicodedata implementation below. BeautifulSoup parses the html and the unicodedata library translates remaining special characters (now unicode rather than HTML) into their literals (e.g. \"\\xa0\" -> \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set of all character entities included in original text, may be able to produce faster implementations:\n",
    "html_char_entities = set()\n",
    "for colname in html_cols:\n",
    "    for rowtext in sites[colname][sites[colname].notna()]:\n",
    "        found_tags = re.findall(r'&.*?;', rowtext)\n",
    "        html_char_entities.update(found_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajsim\\AppData\\Local\\Temp\\ipykernel_6840\\3792983777.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  parsed_text = BeautifulSoup(text, 'html.parser').get_text()\n"
     ]
    }
   ],
   "source": [
    "def html_to_plaintext(text):\n",
    "    parsed_text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    plaintext = normalize('NFKD', parsed_text)\n",
    "    return plaintext\n",
    "\n",
    "for colname in html_cols:\n",
    "    notna_col_entries = sites.loc[sites[colname].notna(), colname]\n",
    "    notna_col_entries = notna_col_entries.map(html_to_plaintext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criteria Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_mapping = {'i': 'C1',\n",
    "                    'ii': 'C2',\n",
    "                    'iii': 'C3',\n",
    "                    'iv': 'C4',\n",
    "                    'v': 'C5',\n",
    "                    'vi': 'C6',\n",
    "                    'vii': 'N7',\n",
    "                    'viii': 'N8',\n",
    "                    'ix': 'N9',\n",
    "                    'x': 'N10'}\n",
    "\n",
    "list_from_parentheses = lambda x: re.findall(r'\\((.*?)\\)', x)\n",
    "all_criteria = sites['criteria_txt'].map(list_from_parentheses).explode()\n",
    "\n",
    "# If any entries extracted from the column are not in the mapping (i.e. not C1-N10) an error is raised.\n",
    "if (min(all_criteria.isin(criteria_mapping.keys()))):\n",
    "    grouped_criteria = all_criteria.groupby([all_criteria.index, all_criteria]).any()\n",
    "    criteria_df = grouped_criteria.unstack(fill_value=False)\n",
    "    criteria_df = criteria_df.rename(columns = criteria_mapping)\n",
    "else:\n",
    "    raise ValueError(\"Given criteria outside of base-mapping table\")\n",
    "\n",
    "sites = sites.assign(**criteria_df)\n",
    "sites = sites.drop(labels='criteria_txt', axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "danger             0.948290\n",
       "iso_code           0.000834\n",
       "justification      0.724771\n",
       "latitude           0.000834\n",
       "location           0.350292\n",
       "longitude          0.000834\n",
       "secondary_dates    0.922435\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns with null entries\n",
    "na_pct = sites.isna().sum() / len(sites)\n",
    "na_pct[na_pct > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>criteria_txt</th>\n",
       "      <th>danger</th>\n",
       "      <th>date_inscribed</th>\n",
       "      <th>extension</th>\n",
       "      <th>http_url</th>\n",
       "      <th>id_number</th>\n",
       "      <th>image_url</th>\n",
       "      <th>iso_code</th>\n",
       "      <th>justification</th>\n",
       "      <th>...</th>\n",
       "      <th>location</th>\n",
       "      <th>longitude</th>\n",
       "      <th>region</th>\n",
       "      <th>revision</th>\n",
       "      <th>secondary_dates</th>\n",
       "      <th>short_description</th>\n",
       "      <th>site</th>\n",
       "      <th>states</th>\n",
       "      <th>transboundary</th>\n",
       "      <th>unique_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Cultural</td>\n",
       "      <td>(iii)(iv)(vi)</td>\n",
       "      <td>None</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>https://whc.unesco.org/en/list/1567</td>\n",
       "      <td>1567</td>\n",
       "      <td>https://whc.unesco.org/uploads/sites/site_1567...</td>\n",
       "      <td>be,fr</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Europe and North America</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>This transnational serial property encompasses...</td>\n",
       "      <td>Funerary and memory sites of the First World W...</td>\n",
       "      <td>Belgium,France</td>\n",
       "      <td>1</td>\n",
       "      <td>2559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    category   criteria_txt danger  date_inscribed  extension   \n",
       "13  Cultural  (iii)(iv)(vi)   None            2023          0  \\\n",
       "\n",
       "                               http_url  id_number   \n",
       "13  https://whc.unesco.org/en/list/1567       1567  \\\n",
       "\n",
       "                                            image_url iso_code justification   \n",
       "13  https://whc.unesco.org/uploads/sites/site_1567...    be,fr          None  \\\n",
       "\n",
       "    ...  location longitude                    region revision   \n",
       "13  ...      None       NaN  Europe and North America        0  \\\n",
       "\n",
       "    secondary_dates                                  short_description   \n",
       "13             None  This transnational serial property encompasses...  \\\n",
       "\n",
       "                                                 site          states   \n",
       "13  Funerary and memory sites of the First World W...  Belgium,France  \\\n",
       "\n",
       "   transboundary  unique_number  \n",
       "13             1           2559  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which entries have null long and lat?\n",
    "sites[np.logical_or(sites['latitude'].isna(), sites['longitude'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transboundary\n",
       "0    1151\n",
       "1      48\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites['transboundary'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
